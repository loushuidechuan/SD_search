# Borrowed heavily from https://github.com/bmaltais/kohya_ss/blob/master/train_db.py and
# https://github.com/ShivamShrirao/diffusers/tree/main/examples/dreambooth
# With some custom bits sprinkled in and some stuff from OG diffusers as well.

import itertools
import json
import logging
import math
import os
import shutil
import time
import traceback
from contextlib import ExitStack
from decimal import Decimal
from pathlib import Path

import safetensors.torch
import tomesd
import torch
import torch.backends.cuda
import torch.backends.cudnn
import torch.nn.functional as F
from accelerate import Accelerator
from accelerate.utils.random import set_seed as set_seed2
from diffusers import (
    AutoencoderKL,
    DiffusionPipeline,
    UNet2DConditionModel,
    DEISMultistepScheduler,
    UniPCMultistepScheduler, StableDiffusionXLPipeline, StableDiffusionPipeline
)
from diffusers.loaders import LoraLoaderMixin, text_encoder_lora_state_dict
from diffusers.models.attention_processor import LoRAAttnProcessor2_0, LoRAAttnProcessor
from diffusers.training_utils import unet_lora_state_dict
from diffusers.utils import logging as dl
from diffusers.utils.torch_utils import randn_tensor
from torch.cuda.profiler import profile
from torch.utils.data import Dataset
from transformers import AutoTokenizer

from dreambooth import shared
from dreambooth.dataclasses.prompt_data import PromptData
from dreambooth.dataclasses.train_result import TrainResult
from dreambooth.dataset.bucket_sampler import BucketSampler
from dreambooth.dataset.sample_dataset import SampleDataset
from dreambooth.deis_velocity import get_velocity
from dreambooth.diff_lora_to_sd_lora import convert_diffusers_to_kohya_lora
from dreambooth.diff_to_sd import compile_checkpoint, copy_diffusion_model
from dreambooth.diff_to_sdxl import compile_checkpoint as compile_checkpoint_xl
from dreambooth.memory import find_executable_batch_size
from dreambooth.optimization import UniversalScheduler, get_optimizer, get_noise_scheduler
from dreambooth.shared import status
from dreambooth.utils.gen_utils import generate_classifiers, generate_dataset
from dreambooth.utils.image_utils import db_save_image, get_scheduler_class
from dreambooth.utils.model_utils import (
    unload_system_models,
    import_model_class_from_model_name_or_path,
    safe_unpickle_disabled,
    xformerify,
    torch2ify
)
from dreambooth.utils.text_utils import encode_hidden_state, save_token_counts
from dreambooth.utils.utils import (cleanup, printm, verify_locon_installed,
                                    patch_accelerator_for_fp16_training)
from dreambooth.webhook import send_training_update
from dreambooth.xattention import optim_to
from helpers.ema_model import EMAModel
from helpers.log_parser import LogParser
from helpers.mytqdm import mytqdm
from lora_diffusion.lora import (
    set_lora_requires_grad,
)

try:
    import wandb

    # ç¦ç”¨çƒ¦äººçš„é­”æ–å¼¹å‡º?
    wandb.config.auto_init = False
except:
    pass

logger = logging.getLogger(__name__)
# å®šä¹‰ä¸€ä¸ªå¤„ç†ç¨‹åºï¼Œå®ƒå°†DEBUGæ¶ˆæ¯æˆ–æ›´é«˜çº§åˆ«çš„æ¶ˆæ¯å†™å…¥sys.stderr
dl.set_verbosity_error()

last_samples = []
last_prompts = []


class ConditionalAccumulator:
    def __init__(self, accelerator, *encoders):
        self.accelerator = accelerator
        self.encoders = encoders
        self.stack = ExitStack()

    def __enter__(self):
        for encoder in self.encoders:
            if encoder is not None:
                self.stack.enter_context(self.accelerator.accumulate(encoder))
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        self.stack.__exit__(exc_type, exc_value, traceback)


"""
#######################################################################################################################
@@                                                                                                                   @@
@@                                                å®šä¹‰ä¸€äº›åŸºç¡€å‡½æ•°                                                    @@
@@                                                                                                                   @@
#######################################################################################################################
"""


# æ£€æŸ¥å’Œè¡¥ä¸è°ƒåº¦ç¨‹åº
def check_and_patch_scheduler(scheduler_class):
    # æ£€æŸ¥è¯¥ç±»æ˜¯å¦å·²ç»æœ‰äº†åä¸º 'get_velocity' çš„æ–¹æ³•ã€‚
    if not hasattr(scheduler_class, 'get_velocity'):
        # æ²¡æœ‰åˆ™æ·»åŠ  get_velocity æ–¹æ³•è¿›å»
        logger.debug(f"Adding 'get_velocity' method to {scheduler_class.__name__}...")
        scheduler_class.get_velocity = get_velocity


try:
    check_and_patch_scheduler(DEISMultistepScheduler)
    check_and_patch_scheduler(UniPCMultistepScheduler)
except:
    logger.warning("Exception while adding 'get_velocity' method to the schedulers.")

export_diffusers = False
user_model_dir = ""

# è®¾ç½®éšæœºç§å­æ˜¯å¦å›ºå®š
def set_seed(deterministic: bool):
    if deterministic:
        torch.backends.cudnn.deterministic = True
        seed = 0
        set_seed2(seed)
    else:
        torch.backends.cudnn.deterministic = False

# å®šä¹‰ä¸€ä¸ªå…¨å±€åˆ—è¡¨
to_delete = []
# æ¸…ç©º to_delete åˆ—è¡¨
def clean_global_state():
    for check in to_delete:
        if check:
            try:
                obj_name = check.__name__
                del check
                # è®°å½•è¢«åˆ é™¤çš„ä¸œè¥¿çš„åç§°
                logger.debug(f"Deleted {obj_name}")
            except:
                pass

# å…ˆéªŒæŸå¤±æƒé‡å‡½æ•°è®¡ç®—
def current_prior_loss(args, current_epoch):
    # ä»¥ä¸‹ä¸‰ä¸ªåˆ¤æ–­æ˜¯è¿›è¡Œåˆå§‹åŒ–åˆ¤æ–­ï¼Œç¡®ä¿åé¢è®¡ç®—æœ‰é»˜è®¤å‚æ•°
    if not args.prior_loss_scale:           # æ£€æµ‹æ˜¯å¦å¼€å¯å…ˆéªŒæŸå¤±è§„æ¨¡
        return args.prior_loss_weight
    if not args.prior_loss_target:          # æ£€æµ‹æ˜¯å¦è®¾ç½®ç›®æ ‡å…ˆéªŒæŸå¤±
        args.prior_loss_target = 150
    if not args.prior_loss_weight_min:      # æ£€æµ‹æ˜¯å¦è®¾ç½®æœ€å°å…ˆéªŒæŸå¤±æƒé‡
        args.prior_loss_weight_min = 0.1

    if current_epoch >= args.prior_loss_target:
        return args.prior_loss_weight_min   # å¦‚æœå¤§äºåˆ™ä½¿ç”¨æœ€å°å…ˆéªŒæŸå¤±æƒé‡
    
    percentage_completed = current_epoch / args.prior_loss_target       # è®¡ç®—å½“å‰è½®æ•°å æ€»è½®æ•°çš„å®Œæˆçš„ç™¾åˆ†æ¯”
    # å…ˆéªŒæŸå¤±æƒé‡éšç€å®Œæˆè½®æ•°å¢å¤šï¼Œæƒé‡é€æ¸å˜å°
    prior = (
            args.prior_loss_weight * (1 - percentage_completed)
            + args.prior_loss_weight_min * percentage_completed
    )
    printm(f"Prior: {prior}")
    return prior

# åœæ­¢ä¸Šä¸‹æ–‡ç®¡ç†å™¨
def stop_profiler(profiler):
    if profiler is not None:
        try:
            # ä¸Šä¼ æ—¥å¿—
            logger.debug("Stopping profiler.")
            profiler.stop()
        except:
            pass
"""
#######################################################################################################################
@@                                                                                                                   @@
@@                                          æ¥æ”¶Webå‚æ•°å¹¶å®ä¾‹åŒ–æ¨¡å‹                                                    @@
@@                                                                                                                   @@
#######################################################################################################################
"""

# è®­ç»ƒä¸»ç¨‹åº
def main(class_gen_method: str = "Native Diffusers", user: str = None) -> TrainResult:
    """
    @param class_gen_method:å›¾åƒç”Ÿæˆåº“ã€‚
    @param user:å‘é€åŸ¹è®­æ›´æ–°çš„ç”¨æˆ·(ç”¨äºæ–°UI)
    @return: è®­ç»ƒç»“æœ
    """
    args = shared.db_model_config
    status_handler = None
    logging_dir = Path(args.model_dir, "logging")
    global export_diffusers, user_model_dir

    # å°è¯•å¯¼å…¥ä¸€äº›åº“ï¼Œå¹¶å®ä¾‹åŒ–ä»–ä»¬
    try:
        from core.handlers.status import StatusHandler
        from core.handlers.config import ConfigHandler
        from core.handlers.models import ModelHandler

        mh = ModelHandler(user_name=user)
        status_handler = StatusHandler(user_name=user, target="dreamProgress")
        export_diffusers = True
        user_model_dir = mh.user_path
        logger.debug(f"Export diffusers: {export_diffusers}, diffusers dir: {user_model_dir}")
        shared.status_handler = status_handler
        logger.debug(f"Loaded config: {args.__dict__}")
    except:
        pass
    
    # å®ä¾‹åŒ–ä¸€ä¸ªæ—¥å¿—ç±»
    log_parser = LogParser()
    
    # æ›´æ–°SDçŠ¶æ€æ çŠ¶æ€çš„æ–¹æ³•
    def update_status(data: dict):
        if status_handler is not None:
            if "iterations_per_second" in data:
                data = {"status": json.dumps(data)}
            status_handler.update(items=data)
            
    # å®šä¹‰äº†ä¸€ä¸ªTrainResultå¯¹è±¡
    result = TrainResult
    # shared.db_model_configä¼ å…¥åˆ°TrainResultçš„configé‡Œ
    result.config = args
    set_seed(args.deterministic)

    @find_executable_batch_size(
        starting_batch_size=args.train_batch_size,
        starting_grad_size=args.gradient_accumulation_steps,
        logging_dir=logging_dir,
        cleanup_function=clean_global_state()
    )

    def inner_loop(train_batch_size: int, gradient_accumulation_steps: int, profiler: profile):
        
        # å®šä¹‰ä¸¤ä¸ªæ–‡æœ¬ç¼–ç å™¨ï¼Œå› ä¸ºSDXLå¤šåŠ äº†ä¸€ä¸ªOpenCLIP ViT-bigG
        text_encoder = None
        text_encoder_two = None
        global last_samples
        global last_prompts
        
        # è®¾ç½®æ–‡æœ¬ç¼–ç å™¨çš„å­¦ä¹ æ­¥æ•°æ¯”ä¾‹ï¼Œä¸º0åˆ™ä¸è®­ç»ƒ
        stop_text_percentage = args.stop_text_encoder
        # å¦‚æœä¸è®­ç»ƒUnetï¼Œåˆ™æ–‡æœ¬ç¼–ç å™¨çš„å­¦ä¹ æ­¥æ•°æ¯”ä¾‹è®¾ä¸º1ï¼Œå…¨ç¨‹å‚ä¸è®­ç»ƒ
        if not args.train_unet:
            stop_text_percentage = 1

        n_workers = 0
        # è®¾ç½®æœ€å¤§è¯å…ƒé•¿åº¦
        args.max_token_length = int(args.max_token_length)
        # å¦‚æœä¸å¡«å……è¯å…ƒä¸”è¯å…ƒé•¿åº¦å¤§äº75ï¼Œå‘å‡ºè­¦å‘Š
        if not args.pad_tokens and args.max_token_length > 75:
            logger.warning("Cannot raise token length limit above 75 when pad_tokens=False")

        verify_locon_installed(args)

        precision = args.mixed_precision if not shared.force_cpu else "no"
        
        # é€‰æ‹©æ··åˆç²¾åº¦
        weight_dtype = torch.float32
        if precision == "fp16":
            weight_dtype = torch.float16
        elif precision == "bf16":
            weight_dtype = torch.bfloat16
            
        # é€‰æ‹©ä½¿ç”¨çš„ç¡¬ä»¶
        try:
            # å°è¯•åˆå§‹åŒ– PyTorch Lighting çš„ Accelerator
            # Accelerator å¯¹è±¡æ˜¯ PyTorch Lightning ä¸­ç”¨äºç®¡ç†åˆ†å¸ƒå¼è®­ç»ƒå’Œç¡¬ä»¶åŠ é€Ÿçš„å®ç”¨å·¥å…·
            accelerator = Accelerator(
                gradient_accumulation_steps=gradient_accumulation_steps,       # æ¢¯åº¦ç´¯ç§¯çš„æ­¥æ•°
                mixed_precision=precision,      # æ··åˆç²¾åº¦è®­ç»ƒçš„å¼€å¯ä¸å…³é—­
                log_with="all",                 # æŒ‡å®šæ—¥å¿—çš„è¯¦ç»†ç¨‹åº¦
                project_dir=logging_dir,        # æŒ‡å®šæ—¥å¿—çš„è¯¦ç»†ç¨‹åº¦
                cpu=shared.force_cpu,           # é€‰æ‹©æ˜¯å¦ä½¿ç”¨CPU
            )

            run_name = "dreambooth.events"
            max_log_size = 250 * 1024  # æŒ‡å®šæœ€å¤§æ—¥å¿—å¤§å°

        except Exception as e:
            if "AcceleratorState" in str(e):
                msg = "Change in precision detected, please restart the webUI entirely to use new precision."
            else:
                msg = f"Exception initializing accelerator: {e}"
            logger.warning(msg)
            result.msg = msg
            result.config = args
            stop_profiler(profiler)
            return result

        # è¿™æ˜¯äºŒçº§çŠ¶æ€æ 
        pbar2 = mytqdm(
            disable=not accelerator.is_local_main_process,
            position=1,
            user=user,
            target="dreamProgress",
            index=1
        )
        #ç›®å‰ï¼Œåœ¨è®­ç»ƒä¸¤ä¸ªæ¨¡å‹æ—¶ä¸å¯èƒ½è¿›è¡Œæ¢¯åº¦ç§¯ç´¯
        #åŠ é€Ÿã€‚è¿™å°†å¾ˆå¿«åŠ é€Ÿå¯ç”¨ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬ä¸å…è®¸æ¢¯åº¦
        #è®­ç»ƒä¸¤ä¸ªæ¨¡å‹æ—¶ç§¯ç´¯ã€‚
        # TODO (patil-suraj):å½“ä¸¤ä¸ªæ¨¡å‹çš„æ¢¯åº¦ç´¯ç§¯åœ¨åŠ é€Ÿä¸­å¯ç”¨æ—¶ï¼Œåˆ é™¤æ­¤æ£€æŸ¥ã€‚
        if (
                stop_text_percentage != 0
                and gradient_accumulation_steps > 1
                and accelerator.num_processes > 1
        ):
            msg = (
                '''
                åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œæ–‡æœ¬ç¼–ç å™¨çš„è®­ç»ƒä¸æ”¯æŒæ¢¯åº¦ç§¯ç´¯ã€‚
                è¯·å°†gradient_accumulation_stepsè®¾ç½®ä¸º1ã€‚è¯¥åŠŸèƒ½å°†åœ¨æœªæ¥å¾—åˆ°æ”¯æŒã€‚æ–‡æœ¬
                ç¼–ç å™¨åŸ¹è®­å°†è¢«ç¦ç”¨ã€‚
                '''
            )
            logger.warning(msg)
            status.textinfo = msg
            update_status({"status": msg})
            stop_text_percentage = 0
        pretrained_path = args.get_pretrained_model_name_or_path()
        logger.debug(f"Pretrained path: {pretrained_path}")
        count, instance_prompts, class_prompts = generate_classifiers(
            args, class_gen_method=class_gen_method, accelerator=accelerator, ui=False, pbar=pbar2
        )

        save_token_counts(args, instance_prompts, 10)

        if status.interrupted:
            result.msg = "Training interrupted."
            stop_profiler(profiler)
            return result

        num_components = 5
        if args.model_type == "SDXL":
            num_components = 7
        pbar2.reset(num_components)
        pbar2.set_description("Loading model components...")
        
        pbar2.set_postfix(refresh=True)
        if class_gen_method == "Native Diffusers" and count > 0:
            unload_system_models()

        def create_vae():
            vae_path = (
                args.pretrained_vae_name_or_path        # æŸ¥çœ‹vaeåç§°æˆ–è·¯å¾„
                if args.pretrained_vae_name_or_path
                else args.get_pretrained_model_name_or_path()       # æ²¡æœ‰åˆ™ä½¿ç”¨getæ–¹æ³•è·å–
            )
            # å…³é—­å®‰å…¨ååºåˆ—
            with safe_unpickle_disabled():
                # é¢„è®­ç»ƒå‚æ•°æ–‡ä»¶åŠ è½½æ¨¡å‹
                new_vae = AutoencoderKL.from_pretrained(    
                    vae_path,
                    subfolder=None if args.pretrained_vae_name_or_path else "vae",      # ç¡®å®šæ˜¯å¦ä½¿ç”¨å­ç›®å½• 
                    revision=args.revision,         # å¯èƒ½æ˜¯æ˜¯æ¨¡å‹çš„ç‰ˆæœ¬æˆ– Git ç‰ˆæœ¬æ§åˆ¶çš„ä¿®è®¢å·
                )
            new_vae.requires_grad_(False)       # ä¸è®¡ç®—æ¢¯åº¦ï¼Œè¿›è¡Œæ¨æ–­
            new_vae.to(accelerator.device, dtype=weight_dtype)
            return new_vae
        
        # åœ¨withè¯­å¥ä¸­å®‰å…¨ååºåˆ—æ˜¯å…³é—­çŠ¶æ€ï¼Œæ‰§è¡Œå®Œwithè¯­å¥å°†é‡æ–°å¼€å¯pytorchçš„å®‰å…¨ååºåˆ—é€‰é¡¹
        with safe_unpickle_disabled():
            # åŠ è½½åˆ†è¯å™¨
            pbar2.set_description("Loading tokenizer...")
            pbar2.update()
            pbar2.set_postfix(refresh=True)
            tokenizer = AutoTokenizer.from_pretrained(
                os.path.join(pretrained_path, "tokenizer"),
                revision=args.revision,
                use_fast=False,
            )

            tokenizer_two = None
            if args.model_type == "SDXL":
                pbar2.set_description("Loading tokenizer 2...")
                pbar2.update()
                pbar2.set_postfix(refresh=True)
                tokenizer_two = AutoTokenizer.from_pretrained(
                    os.path.join(pretrained_path, "tokenizer_2"),
                    revision=args.revision,
                    use_fast=False,
                )

            # å¯¼å…¥æ‰€ä½¿ç”¨çš„æ–‡æœ¬ç¼–ç å™¨
            text_encoder_cls = import_model_class_from_model_name_or_path(
                args.get_pretrained_model_name_or_path(), args.revision
            )
            
            # åŠ è½½è¯åµŒå…¥æ¨¡å‹
            pbar2.set_description("Loading text encoder...")
            pbar2.update()
            pbar2.set_postfix(refresh=True)
            # åŠ è½½æ¨¡å‹å¹¶ä¸ºç¨³å®šæ‰©æ•£åˆ›å»ºåŒ…è£…å™¨
            text_encoder = text_encoder_cls.from_pretrained(
                args.get_pretrained_model_name_or_path(),
                subfolder="text_encoder",
                revision=args.revision,
                torch_dtype=torch.float32,
            )

            if args.model_type == "SDXL":
                # å¯¼å…¥æ­£ç¡®çš„æ–‡æœ¬ç¼–ç å™¨ç±»
                text_encoder_cls_two = import_model_class_from_model_name_or_path(
                    args.get_pretrained_model_name_or_path(), args.revision, subfolder="text_encoder_2"
                )

                pbar2.set_description("Loading text encoder 2...")
                pbar2.update()
                pbar2.set_postfix(refresh=True)
                # åŠ è½½æ¨¡å‹å¹¶ä¸ºç¨³å®šæ‰©æ•£åˆ›å»ºåŒ…è£…å™¨
                text_encoder_two = text_encoder_cls_two.from_pretrained(
                    args.get_pretrained_model_name_or_path(),
                    subfolder="text_encoder_2",
                    revision=args.revision,
                    torch_dtype=torch.float32,
                )

            printm("Created tenc")
            
            # åŠ è½½VAEæ¨¡å‹
            pbar2.set_description("Loading VAE...")
            pbar2.update()
            vae = create_vae()
            printm("Created vae")
            
            # åŠ è½½UNetæ¨¡å‹
            pbar2.set_description("Loading unet...")
            pbar2.update()
            unet = UNet2DConditionModel.from_pretrained(
                args.get_pretrained_model_name_or_path(),
                subfolder="unet",
                revision=args.revision,
                torch_dtype=torch.float32,
            )
            
            # é€‰æ‹©æ³¨æ„åŠ›æ¨¡å‹
            if args.attention == "xformers" and not shared.force_cpu:
                xformerify(unet, use_lora=args.use_lora)
                xformerify(vae, use_lora=args.use_lora)
                
            # æŸ¥çœ‹Pytorchæ˜¯å¦æ”¯æŒç¼–è¯‘ï¼Œå¦‚æœå¯ä»¥åˆ™è¿›è¡Œç¼–è¯‘
            unet = torch2ify(unet)
            
            # å¦‚æœæ˜¯å…¨æ··åˆç²¾åº¦ä¸”ç²¾åº¦ä¸ºfp16ï¼Œåˆ™ç›´æ¥åŠ è½½UNetæ¨¡å‹
            if args.full_mixed_precision:
                if args.mixed_precision == "fp16":
                    patch_accelerator_for_fp16_training(accelerator)
                unet.to(accelerator.device, dtype=weight_dtype)
            else:
                low_precision_error_string = (
                    "Please make sure to always have all model weights in full float32 precision when starting training - "
                    "even if doing mixed precision training. copy of the weights should still be float32."
                )
                
                # æ£€æµ‹UNetæ¨¡å‹ç²¾åº¦æ˜¯ä¸æ˜¯æ··åˆç²¾åº¦
                if accelerator.unwrap_model(unet).dtype != torch.float32:
                    logger.warning(
                        f"Unet loaded as datatype {accelerator.unwrap_model(unet).dtype}. {low_precision_error_string}"
                    )
                    
                # æ£€æµ‹Tokenizeræ¨¡å‹ç²¾åº¦æ˜¯ä¸æ˜¯æ··åˆç²¾åº¦
                if (
                        args.stop_text_encoder != 0
                        and accelerator.unwrap_model(text_encoder).dtype != torch.float32
                ):
                    logger.warning(
                        f"Text encoder loaded as datatype {accelerator.unwrap_model(text_encoder).dtype}."
                        f" {low_precision_error_string}"
                    )
                    
                # æ£€æµ‹word embeddingæ¨¡å‹ç²¾åº¦æ˜¯ä¸æ˜¯æ··åˆç²¾åº¦
                if (
                        args.stop_text_encoder != 0
                        and accelerator.unwrap_model(text_encoder_two).dtype != torch.float32
                ):
                    logger.warning(
                        f"Text encoder loaded as datatype {accelerator.unwrap_model(text_encoder_two).dtype}."
                        f" {low_precision_error_string}"
                    )
                    
            # æŸ¥çœ‹æ˜¯å¦ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
            if args.gradient_checkpointing:
                # å¦‚æœè®­ç»ƒUNetå°±å¯ç”¨æ¢¯åº¦
                if args.train_unet:
                    unet.enable_gradient_checkpointing()
                    
                # æŸ¥çœ‹è®­ç»ƒæ–‡æœ¬ç¼–ç å™¨çš„æ­¥æ•°æ¯”ç‡ï¼Œä¸º0åˆ™ä¸è®­ç»ƒ
                if stop_text_percentage != 0:
                    text_encoder.gradient_checkpointing_enable()
                    if args.model_type == "SDXL":
                        text_encoder_two.gradient_checkpointing_enable()
                    if args.use_lora:
                        # æˆ‘ä»¬éœ€è¦åœ¨ä¸€ä¸ªè¾“å…¥ä¸Šå¯ç”¨æ¢¯åº¦ï¼Œä»¥ä½¿æ¢¯åº¦æ£€æŸ¥ç‚¹å·¥ä½œ
                        # è¿™å°†ä¸ä¼šè¢«ä¼˜åŒ–ï¼Œå› ä¸ºå®ƒä¸æ˜¯ä¼˜åŒ–å™¨çš„å‚æ•°
                        # å¼€å¯åµŒå…¥å±‚çš„æ¢¯åº¦è®¡ç®—
                        text_encoder.text_model.embeddings.position_embedding.requires_grad_(True)
                        if args.model_type == "SDXL":
                            text_encoder_two.text_model.embeddings.position_embedding.requires_grad_(True)
                # ä¸è®­ç»ƒåˆ™ç›´æ¥ä¸¢å…¥CUDA
                else:
                    text_encoder.to(accelerator.device, dtype=weight_dtype)
                    if args.model_type == "SDXL":
                        text_encoder_two.to(accelerator.device, dtype=weight_dtype)
                        
            # é€‰æ‹©æ˜¯å¦è¦å¯åŠ¨ema
            ema_model = None
            if args.use_ema:
                # æŸ¥çœ‹æœ¬åœ°æ˜¯å¦æœ‰emaæ¨¡å‹
                if os.path.exists(
                        os.path.join(
                            args.get_pretrained_model_name_or_path(),
                            "ema_unet",
                            "diffusion_pytorch_model.safetensors",
                        )
                ):
                    # å®ä¾‹åŒ–emaçš„Unetéƒ¨åˆ†
                    ema_unet = UNet2DConditionModel.from_pretrained(
                        args.get_pretrained_model_name_or_path(),
                        subfolder="ema_unet",
                        revision=args.revision,
                        torch_dtype=weight_dtype,
                    )
                    # å¯åŠ¨xformersæ³¨æ„åŠ›æœºåˆ¶ä¸”ä¸åœ¨cp uä¸Šè®¡ç®—
                    if args.attention == "xformers" and not shared.force_cpu:
                        xformerify(ema_unet, use_lora=args.use_lora)

                    ema_model = EMAModel(
                        ema_unet, device=accelerator.device, dtype=weight_dtype
                    )
                    # ç”¨å®Œç›´æ¥åˆ æ‰ï¼ŒèŠ‚çœæ˜¾å­˜
                    del ema_unet
                else:
                    # æ²¡æœ‰åˆ™ç›´æ¥ä½¿ç”¨Unetæ¨¡å‹å®ä¾‹åŒ–ema
                    ema_model = EMAModel(
                        unet, device=accelerator.device, dtype=weight_dtype
                    )

            # åˆ›å»ºå…±äº«çš„unet/tencå­¦ä¹ ç‡å˜é‡
            learning_rate = args.learning_rate
            txt_learning_rate = args.txt_learning_rate
            if args.use_lora:
                learning_rate = args.lora_learning_rate
                txt_learning_rate = args.lora_txt_learning_rate

            # å¦‚æœä¸ä½¿ç”¨Loraä¸”ä¸è®­ç»ƒUnetçš„å…³é—­Unetçš„æ¢¯åº¦
            if args.use_lora or not args.train_unet:
                unet.requires_grad_(False)

            unet_lora_params = None
            
            # é€‰æ‹©æ˜¯å¦ä½¿ç”¨lora
            if args.use_lora:
                pbar2.reset(1)
                pbar2.set_description("Loading LoRA...")
                # ç°åœ¨æˆ‘ä»¬å°†æ·»åŠ æ–°çš„LoRAæƒé‡åˆ°æ³¨æ„å±‚
                # è®¾ç½®æ­£ç¡®çš„loraå±‚
                unet_lora_attn_procs = {}
                unet_lora_params = []
                rank = args.lora_unet_rank

                for name, attn_processor in unet.attn_processors.items():
                    cross_attention_dim = None if name.endswith("attn1.processor") else unet.config.cross_attention_dim
                    hidden_size = None
                    if name.startswith("mid_block"):
                        hidden_size = unet.config.block_out_channels[-1]
                    elif name.startswith("up_blocks"):
                        block_id = int(name[len("up_blocks.")])
                        hidden_size = list(reversed(unet.config.block_out_channels))[block_id]
                    elif name.startswith("down_blocks"):
                        block_id = int(name[len("down_blocks.")])
                        hidden_size = unet.config.block_out_channels[block_id]

                    lora_attn_processor_class = (
                        LoRAAttnProcessor2_0 if hasattr(F, "scaled_dot_product_attention") else LoRAAttnProcessor
                    )
                    if hidden_size is None:
                        logger.warning(f"Could not find hidden size for {name}. Skipping...")
                        continue
                    module = lora_attn_processor_class(
                        hidden_size=hidden_size, cross_attention_dim=cross_attention_dim, rank=rank
                    )
                    unet_lora_attn_procs[name] = module
                    unet_lora_params.extend(module.parameters())

                unet.set_attn_processor(unet_lora_attn_procs)

                # æ–‡æœ¬ç¼–ç å™¨æ¥è‡ªğŸ¤—å˜å‹å™¨ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸èƒ½ç›´æ¥ä¿®æ”¹å®ƒã€‚
                # æ‰€ä»¥ï¼Œç›¸åï¼Œæˆ‘ä»¬monkey-patchå®ƒçš„æ³¨æ„åŠ›å—çš„å‰å‘ä¼ æ’­ã€‚
                if stop_text_percentage != 0:
                    # ç¡®ä¿dtypeä¸ºfloat32ï¼Œå³ä½¿åœ¨fp16ä¸­åŠ è½½äº†æœªè®­ç»ƒçš„æ¨¡å‹çš„å…¶ä½™éƒ¨åˆ†
                    text_encoder_lora_params = LoraLoaderMixin._modify_text_encoder(
                        text_encoder, dtype=torch.float32, rank=args.lora_txt_rank
                    )

                    if args.model_type == "SDXL":
                        text_encoder_lora_params_two = LoraLoaderMixin._modify_text_encoder(
                            text_encoder_two, dtype=torch.float32, rank=args.lora_txt_rank
                        )
                        params_to_optimize = (
                            itertools.chain(unet_lora_params, text_encoder_lora_params, text_encoder_lora_params_two))
                    else:
                        params_to_optimize = (itertools.chain(unet_lora_params, text_encoder_lora_params))

                else:
                    params_to_optimize = unet_lora_params

                # å¦‚æœæŒ‡å®šï¼ŒåŠ è½½LoRAæƒé‡
                if args.lora_model_name is not None and args.lora_model_name != "":
                    logger.debug(f"Load lora from {args.lora_model_name}")
                    lora_state_dict, network_alphas = LoraLoaderMixin.lora_state_dict(args.lora_model_name)
                    LoraLoaderMixin.load_lora_into_unet(lora_state_dict, network_alphas=network_alphas, unet=unet)

                    LoraLoaderMixin.load_lora_into_text_encoder(
                        lora_state_dict, network_alphas=network_alphas, text_encoder=text_encoder)
                    if text_encoder_two is not None:
                        LoraLoaderMixin.load_lora_into_text_encoder(
                            lora_state_dict, network_alphas=network_alphas, text_encoder=text_encoder_two)
                        
                        
            # CLIP/UnetäºŒé€‰ä¸€
            elif stop_text_percentage != 0:
                if args.train_unet:
                    if args.model_type == "SDXL":
                        # åˆ›é€ ä¸€ä¸ªæ‹¥æœ‰Unetå’ŒCLIPæ‰€æœ‰å¯è®­ç»ƒå‚æ•°çš„è¿­ä»£å™¨
                        params_to_optimize = itertools.chain(unet.parameters(), text_encoder.parameters(),
                                                             text_encoder_two.parameters())
                    else:
                        # åˆ›é€ ä¸€ä¸ªæ‹¥æœ‰Unetå’ŒCLIPæ‰€æœ‰å¯è®­ç»ƒå‚æ•°çš„è¿­ä»£å™¨
                        params_to_optimize = itertools.chain(unet.parameters(), text_encoder.parameters())
                else:
                    if args.model_type == "SDXL":
                        # åˆ›é€ ä¸€ä¸ªæ‹¥æœ‰Unetå’ŒCLIPæ‰€æœ‰å¯è®­ç»ƒå‚æ•°çš„è¿­ä»£å™¨
                        params_to_optimize = itertools.chain(text_encoder.parameters(), text_encoder_two.parameters())
                    else:
                        # åˆ›é€ ä¸€ä¸ªæ‹¥æœ‰Unetå’ŒCLIPæ‰€æœ‰å¯è®­ç»ƒå‚æ•°çš„è¿­ä»£å™¨
                        params_to_optimize = itertools.chain(text_encoder.parameters())
            else:
                params_to_optimize = unet.parameters()
                
            # é€‰æ‹©ä¼˜åŒ–å™¨
            optimizer = get_optimizer(args.optimizer, learning_rate, args.weight_decay, params_to_optimize)
            if len(optimizer.param_groups) > 1:
                try:
                    # åµŒå…¥å±‚æƒé‡è¡°å‡
                    optimizer.param_groups[1]["weight_decay"] = args.tenc_weight_decay
                    # è£å‰ªåµŒå…¥å±‚æ¢¯åº¦å½’ä¸€å±‚
                    optimizer.param_groups[1]["grad_clip_norm"] = args.tenc_grad_clip_norm
                except:
                    logger.warning("Exception setting tenc weight decay")
                    traceback.print_exc()

            if len(optimizer.param_groups) > 2:
                try:
                    # XLåµŒå…¥å±‚æƒé‡è¡°å‡
                    optimizer.param_groups[2]["weight_decay"] = args.tenc_weight_decay
                    # XLè£å‰ªåµŒå…¥å±‚æ¢¯åº¦å½’ä¸€å±‚
                    optimizer.param_groups[2]["grad_clip_norm"] = args.tenc_grad_clip_norm
                except:
                    logger.warning("Exception setting tenc weight decay")
                    traceback.print_exc()
                    
            # è®¾ç½®å›¾åƒç”Ÿæˆè°ƒåº¦å™¨
            noise_scheduler = get_noise_scheduler(args)
            global to_delete
            to_delete = [unet, text_encoder, text_encoder_two, tokenizer, tokenizer_two, optimizer, vae]
            # æ¸…ç†å†…å­˜
            def cleanup_memory():
                try:
                    if unet:
                        del unet
                    if text_encoder:
                        del text_encoder
                    if text_encoder_two:
                        del text_encoder_two
                    if tokenizer:
                        del tokenizer
                    if tokenizer_two:
                        del tokenizer_two
                    if optimizer:
                        del optimizer
                    if train_dataloader:
                        del train_dataloader
                    if train_dataset:
                        del train_dataset
                    if lr_scheduler:
                        del lr_scheduler
                    if vae:
                        del vae
                    if unet_lora_params:
                        del unet_lora_params
                except:
                    pass
                cleanup(True)
                
            # é€‰æ‹©æ˜¯å¦ç¼“å­˜æ½œåœ¨å˜é‡
            if args.cache_latents:
                vae.to(accelerator.device, dtype=weight_dtype)
                vae.requires_grad_(False)
                vae.eval()
                
            # æ£€æµ‹çŠ¶æ€æ çš„ä¸­æ­¢æŒ‰é’®æ˜¯å¦è¢«ç‚¹å‡»è¿‡
            if status.interrupted:
                result.msg = "Training interrupted."
                stop_profiler(profiler)
                return result

            printm("Loading dataset...")
            pbar2.reset()
            pbar2.set_description("Loading dataset")
            
            # å°†å…ˆå‰ä¿å­˜å…³é—­
            with_prior_preservation = False
            # èµ‹å€¼tokenizerså¹¶åˆ¤æ–­è®­ç»ƒæ¨¡å‹æ˜¯å¦ä¸ºXLæ¨¡å‹
            tokenizers = [tokenizer] if tokenizer_two is None else [tokenizer, tokenizer_two]
            # èµ‹å€¼text_encoderså¹¶åˆ¤æ–­è®­ç»ƒæ¨¡å‹æ˜¯å¦ä¸ºXLæ¨¡å‹
            text_encoders = [text_encoder] if text_encoder_two is None else [text_encoder, text_encoder_two]
            # è¿”å›ä¸€ä¸ªDBæ•°æ®é›†ç±»ï¼Œå¹¶è¿›è¡Œèµ‹å€¼
            train_dataset = generate_dataset(
                model_name=args.model_name,
                instance_prompts=instance_prompts,
                class_prompts=class_prompts,
                batch_size=args.train_batch_size,
                tokenizer=tokenizers,
                text_encoder=text_encoders,
                accelerator=accelerator,
                vae=vae if args.cache_latents else None,
                debug=False,
                model_dir=args.model_dir,
                max_token_length=args.max_token_length,
                pbar=pbar2
            )
            # å¦‚æœè®­ç»ƒæ•°æ®é›†ç±»é‡Œçš„ç±»æ•°é‡å¤§äº0
            if train_dataset.class_count > 0:
                # å°†å…ˆå‰ä¿å­˜å¼€å¯
                with_prior_preservation = True
            pbar2.reset()
            printm("Dataset loaded.")
            # å°†æœ€å¤§è¯å…ƒè¿›è¡Œèµ‹å€¼
            tokenizer_max_length = tokenizer.model_max_length
            # å¦‚æœè¦ç¼“å­˜æ½œåœ¨å˜é‡
            if args.cache_latents:
                printm("Unloading vae.")
                del vae
                # ä¿ç•™å¯¹vaeçš„å¼•ç”¨ä»¥ä¾›ä»¥åæ£€æŸ¥
                vae = None
                # TODO:å°è¯•åœ¨è¿™é‡Œå¸è½½æ ‡è®°å™¨?
                del tokenizer
                if tokenizer_two is not None:
                    del tokenizer_two
                tokenizer = None
                tokenizer2 = None
                
            # æ£€æµ‹çŠ¶æ€æ çš„ä¸­æ­¢æŒ‰é’®æ˜¯å¦è¢«ç‚¹å‡»è¿‡
            if status.interrupted:
                result.msg = "Training interrupted."
                stop_profiler(profiler)
                return result
            
            # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦ä¸ºç©º
            if train_dataset.__len__ == 0:
                msg = "Please provide a directory with actual images in it."
                logger.warning(msg)
                status.textinfo = msg
                update_status({"status": status})
                cleanup_memory()
                result.msg = msg
                result.config = args
                stop_profiler(profiler)
                return result

            def collate_fn_db(examples):
                # ä»examplesæå– input_ids ã€å›¾åƒã€çˆ¶ç±»å‹ã€æƒé‡
                input_ids = [example["input_ids"] for example in examples]
                pixel_values = [example["image"] for example in examples]
                types = [example["is_class"] for example in examples]
                weights = [
                    current_prior_loss_weight if example["is_class"] else 1.0
                    for example in examples
                ]
                # è®¾ç½®æŸå¤±ä¸º0
                loss_avg = 0
                # è®¡ç®—æŸå¤±å¹³å‡å€¼
                for weight in weights:
                    loss_avg += weight
                loss_avg /= len(weights)
                # å°†å›¾åƒå †å ä¸ºå¼ é‡
                pixel_values = torch.stack(pixel_values)
                # å¦‚æœä¸ç¼“å­˜æ½œå˜é‡ï¼Œå°±æŠŠå›¾åƒè½¬æ¢ä¸ºæµ®ç‚¹å¼ é‡
                if not args.cache_latents:
                    pixel_values = pixel_values.to(
                        memory_format=torch.contiguous_format
                    ).float()
                # å°†å›¾ç‰‡è¿›è¡Œåˆ—æ‹¼æ¥ï¼Œå˜æˆä¸€ä¸ªå¼ é‡
                input_ids = torch.cat(input_ids, dim=0)
                
                # ä¸€ä¸ªæ‰¹æ¬¡çš„å›¾åƒæ•°æ®
                batch_data = {
                    "input_ids": input_ids,
                    "images": pixel_values,
                    "types": types,
                    "loss_avg": loss_avg,
                }
                # å¦‚æœå‚æ•°é‡Œè¿˜æœ‰input_ids2ï¼Œåˆ™ä¸ºå­—å…¸æ·»åŠ é¢å¤–çš„å­—å…¸åˆ°batch_data
                if "input_ids2" in examples[0]:
                    input_ids_2 = [example["input_ids2"] for example in examples]
                    input_ids_2 = torch.stack(input_ids_2)

                    batch_data["input_ids2"] = input_ids_2
                    batch_data["original_sizes_hw"] = torch.stack(
                        [torch.LongTensor(x["original_sizes_hw"]) for x in examples])
                    batch_data["crop_top_lefts"] = torch.stack([torch.LongTensor(x["crop_top_lefts"]) for x in examples])
                    batch_data["target_sizes_hw"] = torch.stack([torch.LongTensor(x["target_sizes_hw"]) for x in examples])
                return batch_data

            def collate_fn_sdxl(examples):
                # ä»examplesè·å–input_idsã€å›¾åƒã€æ–‡æœ¬ç¼–ç å™¨ã€æ—¶é—´æ­¥
                input_ids = [example["input_ids"] for example in examples if not example["is_class"]]
                pixel_values = [example["image"] for example in examples if not example["is_class"]]
                add_text_embeds = [example["instance_added_cond_kwargs"]["text_embeds"] for example in examples if
                                   not example["is_class"]]
                add_time_ids = [example["instance_added_cond_kwargs"]["time_ids"] for example in examples if
                                not example["is_class"]]

                # Concatç±»å’Œå®ä¾‹ç¤ºä¾‹ä»¥ä¿ç•™ä¹‹å‰çš„å†…å®¹ã€‚
                # è¿™æ ·åšæ˜¯ä¸ºäº†é¿å…ä¸¤æ¬¡å‘å‰ä¼ é€’ã€‚
                if with_prior_preservation:
                    input_ids += [example["input_ids"] for example in examples if example["is_class"]]
                    pixel_values += [example["image"] for example in examples if example["is_class"]]
                    add_text_embeds += [example["instance_added_cond_kwargs"]["text_embeds"] for example in examples if
                                        example["is_class"]]
                    add_time_ids += [example["instance_added_cond_kwargs"]["time_ids"] for example in examples if
                                     example["is_class"]]
                    
                # å°†å›¾ç‰‡è¿›è¡Œåˆ—æ‹¼æ¥ï¼Œå˜æˆä¸€ä¸ªå¼ é‡ï¼Œå¹¶è½¬åŒ–ä¸ºæµ®ç‚¹æ•°
                pixel_values = torch.stack(pixel_values)
                pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()
                
                # æŠŠinput_idsã€æ–‡æœ¬ç¼–ç å™¨ã€æ—¶é—´æ­¥å˜ä¸ºä¸€ä¸ªå¼ é‡
                input_ids = torch.cat(input_ids, dim=0)
                add_text_embeds = torch.cat(add_text_embeds, dim=0)
                add_time_ids = torch.cat(add_time_ids, dim=0)
                
                # å®šä¹‰æ‰¹æ•°æ®
                batch = {
                    "input_ids": input_ids,
                    "images": pixel_values,
                    "unet_added_conditions": {"text_embeds": add_text_embeds, "time_ids": add_time_ids},
                }

                return batch
            
            # æ ¹æ®train_batch_sizeè¿›è¡Œæ•°æ®é›†åˆ†æ‰¹
            sampler = BucketSampler(train_dataset, train_batch_size)
            
            # æ ¹æ®æ¨¡å‹ä½¿ç”¨ä¸åŒæ•´ç†è§„èŒƒ
            collate_fn = collate_fn_db
            if args.model_type == "SDXL":
                collate_fn = collate_fn_sdxl
            
            train_dataloader = torch.utils.data.DataLoader(
                train_dataset,                  # è®­ç»ƒé›†å®ä¾‹
                batch_size=1,                   # æ‰¹æ¬¡å¤§å°
                batch_sampler=sampler,          # è®¾ç½®æ‰¹é‡‡æ ·å™¨
                collate_fn=collate_fn,          # ä½¿ç”¨æ¨¡å‹çš„æ•´ç†è§„èŒƒï¼Œç”¨äºç»„è£…å°æ‰¹é‡æ•°æ®
                num_workers=n_workers,          # è®¾ç½®åŠ è½½å™¨å·¥ä½œçº¿ç¨‹æ•°é‡ï¼Œ0ä¸ºåªç”¨ä¸»ç¨‹åºåŠ è½½
            )
            
            # è®¡ç®—æœ€å¤§è®­ç»ƒæ­¥æ•°ï¼šè®­ç»ƒè½®æ•° * è®­ç»ƒæ•°æ®ç±»é•¿åº¦
            max_train_steps = args.num_train_epochs * len(train_dataset)

            # è¿™æ˜¯ç‹¬ç«‹çš„ï¼Œå› ä¸ºä¼˜åŒ–å™¨ã€‚Stepåœ¨è®­ç»ƒä¸­æ¯ä¸ªâ€œStepâ€åªè¢«è°ƒç”¨ä¸€æ¬¡ï¼Œæ‰€ä»¥å®ƒä¸æ˜¯
            # å—æ‰¹é‡å¤§å°çš„å½±å“
            # è®¡ç®—é¢„å®šè®­ç»ƒæ­¥æ•°ï¼šè®­ç»ƒè½®æ¬¡ * æ•°æ®é›†æ•°é‡
            sched_train_steps = args.num_train_epochs * train_dataset.num_train_images

            lr_scale_pos = args.lr_scale_pos
            if class_prompts:
                lr_scale_pos *= 2
                
            # è®¾ç½®å­¦ä¹ è°ƒåº¦å™¨
            lr_scheduler = UniversalScheduler(
                name=args.lr_scheduler,
                optimizer=optimizer,                        # ä¼˜åŒ–å™¨
                num_warmup_steps=args.lr_warmup_steps,      # å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•°
                total_training_steps=sched_train_steps,     # è®­ç»ƒæ€»æ­¥æ•°
                min_lr=args.learning_rate_min,              # æœ€å°å­¦ä¹ ç‡
                total_epochs=args.num_train_epochs,         # æ€»è½®æ•°
                num_cycles=args.lr_cycles,
                power=args.lr_power,
                factor=args.lr_factor,
                scale_pos=lr_scale_pos,
                unet_lr=learning_rate,                      # Unetå­¦ä¹ ç‡
                tenc_lr=txt_learning_rate,                  # æ–‡æœ¬å­¦ä¹ ç‡
            )
            
            
            # å°†æ¨¡å‹å’Œæ•°æ®åŠ è½½å™¨ç§»åŠ¨åˆ°ç‰¹å®šè®¾å¤‡ä¸Šï¼Œå¹¶è¿›è¡Œèµ‹å€¼
            # åˆ›é€  ema, é˜²æ­¢ OOMï¼ˆçˆ†æ˜¾å­˜çš„æ„æ€
            if args.use_ema:
                # å¦‚æœè®­ç»ƒCLIP
                if stop_text_percentage != 0:
                    (
                        ema_model.model,
                        unet,
                        text_encoder,
                        optimizer,
                        train_dataloader,
                        lr_scheduler,
                    ) = accelerator.prepare(
                        ema_model.model,
                        unet,
                        text_encoder,
                        optimizer,
                        train_dataloader,
                        lr_scheduler,
                    )
                else:
                    (
                        ema_model.model,
                        unet,
                        optimizer,
                        train_dataloader,
                        lr_scheduler,
                    ) = accelerator.prepare(
                        ema_model.model, unet, optimizer, train_dataloader, lr_scheduler
                    )
            # ä¸ä½¿ç”¨ema
            else:
                # å¦‚æœè®­ç»ƒCLIP
                if stop_text_percentage != 0:
                    (
                        unet,
                        text_encoder,
                        optimizer,
                        train_dataloader,
                        lr_scheduler,
                    ) = accelerator.prepare(
                        unet, text_encoder, optimizer, train_dataloader, lr_scheduler
                    )
                else:
                    unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
                        unet, optimizer, train_dataloader, lr_scheduler
                    )
                    
            # å¦‚æœç¼“å­˜æ½œåœ¨å˜é‡ä¸”æœ‰vae
            if not args.cache_latents and vae is not None:
                vae.to(accelerator.device, dtype=weight_dtype)

            if stop_text_percentage == 0:
                text_encoder.to(accelerator.device, dtype=weight_dtype)
            # ä¹‹åï¼Œæˆ‘ä»¬é‡æ–°è®¡ç®—è®­ç»ƒæ­¥æ•°
            # æˆ‘ä»¬éœ€è¦åˆå§‹åŒ–æˆ‘ä»¬ä½¿ç”¨çš„è¿½è¸ªå™¨ï¼Œå¹¶å­˜å‚¨æˆ‘ä»¬çš„é…ç½®ã€‚
            # is_main_processæ–¹æ³•æ˜¯ç”¨æ¥æ‰§è¡Œåªæ‰§è¡Œä¸€æ¬¡çš„è¯­å¥
            if accelerator.is_main_process:
                accelerator.init_trackers("dreambooth")

            """
            #######################################################################################################################
            @@                                                                                                                   @@
            @@                                         è®­ç»ƒå‚æ•°é…ç½®å¹¶å®šä¹‰ä¿å­˜æ¨¡å‹å‡½æ•°                                               @@
            @@                                                                                                                   @@
            #######################################################################################################################
            """
            total_batch_size = (
                    train_batch_size * accelerator.num_processes * gradient_accumulation_steps
            )
            max_train_epochs = args.num_train_epochs
            # æˆ‘ä»¬è®¡ç®—æ–‡æœ¬ç¼–ç å™¨çš„è®­ç»ƒæ­¥æ•°ï¼ˆæœ€å¤§è®­ç»ƒè½®æ•° * åœæ­¢ç™¾åˆ†æ¯”ï¼‰
            text_encoder_epochs = round(max_train_epochs * stop_text_percentage)
            global_step = 0             # å…¨å±€æ­¥æ•°
            global_epoch = 0            # å…¨å±€è½®æ•°
            session_epoch = 0           # å½“å‰è½®æ•°  
            first_epoch = 0             # ä¸€è½®
            resume_step = 0             # æ¢å¤æ­¥æ•°
            last_model_save = 0         # æœ€åä¸€ä¸ªä¿å­˜æ¨¡å‹çš„è½®æ•°
            last_image_save = 0         # æœ€åä¸€ä¸ªä¿å­˜å›¾ç‰‡çš„è½®æ•°
            resume_from_checkpoint = False
            new_hotness = os.path.join(
                args.model_dir, "checkpoints", f"checkpoint-{args.snapshot}"
            )
            if os.path.exists(new_hotness):
                logger.debug(f"Resuming from checkpoint {new_hotness}")

                try:
                    # å¯¼å…¥modules.sharedåº“
                    import modules.shared
                    # å°†modulesåº“é‡Œçš„å®‰å…¨ååºåˆ—åŒ–éƒ¨åˆ†èµ‹å€¼ç»™no_safe
                    no_safe = modules.shared.cmd_opts.disable_safe_unpickle
                    # å°†modules.sharedå®‰å…¨ååºåˆ—åŒ–å¼€å¯
                    modules.shared.cmd_opts.disable_safe_unpickle = True
                except:
                    no_safe = False

                try:
                    import modules.shared
                    # åŠ è½½æ£€æŸ¥ç‚¹
                    accelerator.load_state(new_hotness)
                    # è®¾ç½®å®‰å…¨ååºåˆ—åŒ–çš„çŠ¶æ€
                    modules.shared.cmd_opts.disable_safe_unpickle = no_safe
                    # è®¾ç½®å…¨å±€æ­¥æ•°å’Œæ¢å¤æ­¥æ•°ã€‚
                    global_step = resume_step = args.revision
                    # è®¾ç½®æ˜¯å¦ä»æ£€æŸ¥ç‚¹å›å¤
                    resume_from_checkpoint = True
                    # è®¾ç½®æ€»è½®æ•°
                    first_epoch = args.lifetime_epoch
                    global_epoch = args.lifetime_epoch
                except Exception as lex:
                    logger.warning(f"Exception loading checkpoint: {lex}")
            # æ˜¾ç¤ºè®­ç»ƒé…ç½®
            logger.debug("  ***** Running training *****")
            if shared.force_cpu:
                logger.debug(f"  TRAINING WITH CPU ONLY")
            logger.debug(f"  Num batches each epoch = {len(train_dataset) // train_batch_size}")
            logger.debug(f"  Num Epochs = {max_train_epochs}")
            logger.debug(f"  Batch Size Per Device = {train_batch_size}")
            logger.debug(f"  Gradient Accumulation steps = {gradient_accumulation_steps}")
            logger.debug(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
            logger.debug(f"  Text Encoder Epochs: {text_encoder_epochs}")
            logger.debug(f"  Total optimization steps = {sched_train_steps}")
            logger.debug(f"  Total training steps = {max_train_steps}")
            logger.debug(f"  Resuming from checkpoint: {resume_from_checkpoint}")
            logger.debug(f"  First resume epoch: {first_epoch}")
            logger.debug(f"  First resume step: {resume_step}")
            logger.debug(f"  Lora: {args.use_lora}, Optimizer: {args.optimizer}, Prec: {precision}")
            logger.debug(f"  Gradient Checkpointing: {args.gradient_checkpointing}")
            logger.debug(f"  EMA: {args.use_ema}")
            logger.debug(f"  UNET: {args.train_unet}")
            logger.debug(f"  Freeze CLIP Normalization Layers: {args.freeze_clip_normalization}")
            logger.debug(f"  LR{' (Lora)' if args.use_lora else ''}: {learning_rate}")
            if stop_text_percentage > 0:
                logger.debug(f"  Tenc LR{' (Lora)' if args.use_lora else ''}: {txt_learning_rate}")
            logger.debug(f"  V2: {args.v2}")
            
            # å°†ç¯å¢ƒå˜é‡é‡Œçš„ CUDA_LAUNCH_BLOCKING è®¾ç½®ä¸º1 
            # å¯åŠ¨ CUDA çš„æ—¶å€™è¿›è¡Œé˜»å¡ã€‚åœ¨ GPU æ‰§è¡Œå®Œå‡½æ•°ä¹‹å‰ï¼ŒCPUå¤„äºé˜»å¡çŠ¶æ€ï¼Œå¤šç”¨äºè°ƒè¯•ç¨‹åºï¼Œæ£€æµ‹æ½œåœ¨é—®é¢˜
            os.environ.__setattr__("CUDA_LAUNCH_BLOCKING", 1)
            
            # æ£€æŸ¥ä¿å­˜
            def check_save(is_epoch_check=False):
                # è·å–åˆ°ä¸Šä¸€å±‚çš„å˜é‡
                nonlocal last_model_save
                nonlocal last_image_save
                # è¿›è¡Œä¼ å‚ï¼Œå’Œå‚æ•°åˆå§‹åŒ–
                save_model_interval = args.save_embedding_every             # ä¿å­˜æ¨¡å‹é—´éš”
                save_image_interval = args.save_preview_every               # ä¿å­˜å›¾åƒé—´éš”
                save_completed = session_epoch >= max_train_epochs          # åˆ¤æ–­å½“å‰è½®æ•°æ˜¯å¦å¤§äºæœ€å¤§è½®æ•°
                save_canceled = status.interrupted                          # æŸ¥çœ‹æ˜¯å¦å·²ä¸­æ­¢ç¨‹åº
                save_image = False
                save_model = False
                save_lora = False
                
                # å¦‚æœæ²¡æœ‰ä¸­æ­¢ä¸”å½“å‰è½®æ•°å°äºæœ€å¤§è½®æ•°
                if not save_canceled and not save_completed:
                    # å®šç‚¹ä¿å­˜æ¨¡å‹
                    if 0 < save_model_interval <= session_epoch - last_model_save:
                        save_model = True
                        # å¦‚æœä½¿ç”¨lora
                        if args.use_lora:
                            save_lora = True
                        # è®°å½•ä¿å­˜è½®æ•°
                        last_model_save = session_epoch

                    # å®šç‚¹ä¿å­˜å›¾ç‰‡
                    if 0 < save_image_interval <= session_epoch - last_image_save:
                        save_image = True
                        # è®°å½•ä¿å­˜è½®æ•°
                        last_image_save = session_epoch
                        
                # å¦åˆ™è¿›è¡Œæ¨¡å‹å’Œå›¾åƒä¿å­˜
                else:
                    logger.debug("\nSave completed/canceled.")
                    if global_step > 0:
                        save_image = True
                        save_model = True
                        if args.use_lora:
                            save_lora = True
                            
                # åˆå§‹åŒ–ä¿å­˜å¿«ç…§æ“ä½œ
                save_snapshot = False
                
                # æ˜¯å¦æ£€æŸ¥ä¿å­˜
                if is_epoch_check:
                    # å¦‚æœä¿å­˜ æ ·æœ¬ çŠ¶æ€ä¸ºTrue
                    if shared.status.do_save_samples:
                        save_image = True
                        shared.status.do_save_samples = False
                    # å¦‚æœä¿å­˜ æ¨¡å‹ çŠ¶æ€ä¸ºTrue
                    if shared.status.do_save_model:
                        if args.use_lora:
                            save_lora = True
                        save_model = True
                        shared.status.do_save_model = False

                save_checkpoint = False
                if save_model:
                    # å¦‚æœå–æ¶ˆè®­ç»ƒ
                    if save_canceled:
                        # å¦‚æœè®­ç»ƒæ­¥æ•°å¤§äº0
                        if global_step > 0:
                            logger.debug("Canceled, enabling saves.")
                            save_snapshot = args.save_state_cancel
                            save_checkpoint = args.save_ckpt_cancel
                    # å¦‚æœè®­ç»ƒå·²ç»å®Œæˆ
                    elif save_completed:
                        if global_step > 0:
                            logger.debug("Completed, enabling saves.")
                            save_snapshot = args.save_state_after
                            save_checkpoint = args.save_ckpt_after
                    # è®­ç»ƒä¸­
                    else:
                        save_snapshot = args.save_state_during
                        save_checkpoint = args.save_ckpt_during
                    if save_checkpoint and args.use_lora:
                        save_checkpoint = False
                        save_lora = True
                # å¦‚æœä½¿ç”¨lora
                if not args.use_lora:
                    save_lora = False
                    
                # å¦‚æœæœ‰è¦ä¿å­˜çš„æ¨¡å‹
                if (
                        save_checkpoint
                        or save_snapshot
                        or save_lora
                        or save_image
                        or save_model
                ):
                    # è°ƒç”¨ä¿å­˜æ¨¡å‹æ–¹æ³•
                    save_weights(
                        save_image,
                        save_model,
                        save_snapshot,
                        save_checkpoint,
                        save_lora
                    )

                return save_model, save_image
            
            # ä¿å­˜æ¨¡å‹æ–¹æ³•
            def save_weights(
                    save_image, save_diffusers, save_snapshot, save_checkpoint, save_lora
            ):
                global last_samples
                global last_prompts
                nonlocal vae
                nonlocal pbar2

                printm(" Saving weights.")
                pbar2.reset()
                pbar2.set_description("Saving weights/samples...")
                pbar2.set_postfix(refresh=True)

                # ä½¿ç”¨ç»è¿‡è®­ç»ƒçš„æ¨¡å—åˆ›å»ºç®¡é“å¹¶ä¿å­˜å®ƒã€‚
                if accelerator.is_main_process:
                    printm("Pre-cleanup.")
                    torch_rng_state = None
                    cuda_gpu_rng_state = None
                    cuda_cpu_rng_state = None
                    # ä¿å­˜éšæœºçŠ¶æ€ï¼Œè¿™æ ·æ ·æœ¬ç”Ÿæˆä¸ä¼šå½±å“è®­ç»ƒã€‚
                    if shared.device.type == 'cuda':
                        torch_rng_state = torch.get_rng_state()
                        cuda_gpu_rng_state = torch.cuda.get_rng_state(device="cuda")
                        cuda_cpu_rng_state = torch.cuda.get_rng_state(device="cpu")

                    optim_to(profiler, optimizer)

                    if profiler is None:
                        cleanup()

                    if vae is None:
                        printm("Loading vae.")
                        vae = create_vae()

                    printm("Creating pipeline.")
                    if args.model_type == "SDXL":
                        s_pipeline = StableDiffusionXLPipeline.from_pretrained(
                            args.get_pretrained_model_name_or_path(),
                            unet=accelerator.unwrap_model(unet, keep_fp32_wrapper=True),
                            text_encoder=accelerator.unwrap_model(
                                text_encoder, keep_fp32_wrapper=True
                            ),
                            text_encoder_2=accelerator.unwrap_model(
                                text_encoder_two, keep_fp32_wrapper=True
                            ),
                            vae=vae.to(accelerator.device),
                            torch_dtype=weight_dtype,
                            revision=args.revision,
                        )
                        xformerify(s_pipeline.unet,use_lora=args.use_lora)
                    else:
                        s_pipeline = DiffusionPipeline.from_pretrained(
                            args.get_pretrained_model_name_or_path(),
                            unet=accelerator.unwrap_model(unet, keep_fp32_wrapper=True),
                            text_encoder=accelerator.unwrap_model(
                                text_encoder, keep_fp32_wrapper=True
                            ),
                            vae=vae,
                            torch_dtype=weight_dtype,
                            revision=args.revision,
                        )
                        xformerify(s_pipeline.unet,use_lora=args.use_lora)
                        xformerify(s_pipeline.vae,use_lora=args.use_lora)

                    weights_dir = args.get_pretrained_model_name_or_path()

                    if user_model_dir != "":
                        loras_dir = os.path.join(user_model_dir, "Lora")
                    else:
                        model_dir = shared.models_path
                        loras_dir = os.path.join(model_dir, "Lora")
                    delete_tmp_lora = False
                    # å¦‚æœæˆ‘ä»¬åªéœ€è¦ä¿å­˜å›¾åƒï¼Œè¯·æ›´æ–°ä¸´æ—¶è·¯å¾„
                    if save_image:
                        logger.debug("Save image is set.")
                        if args.use_lora:
                            if not save_lora:
                                logger.debug("Saving lora weights instead of checkpoint, using temp dir.")
                                save_lora = True
                                delete_tmp_lora = True
                                save_checkpoint = False
                                save_diffusers = False
                                os.makedirs(loras_dir, exist_ok=True)
                        elif not save_diffusers:
                            logger.debug("Saving checkpoint, using temp dir.")
                            save_diffusers = True
                            weights_dir = f"{weights_dir}_temp"
                            os.makedirs(weights_dir, exist_ok=True)
                        else:
                            save_lora = False
                            logger.debug(f"Save checkpoint: {save_checkpoint} save lora {save_lora}.")
                    # è¿™é‡Œéœ€è¦inference_mode()æ¥é˜²æ­¢ä¿å­˜æ—¶å‡ºç°é—®é¢˜å—?
                    logger.debug(f"Loras dir: {loras_dir}")

                    # è®¾ç½®ptè·¯å¾„
                    if args.custom_model_name == "":
                        lora_model_name = args.model_name
                    else:
                        lora_model_name = args.custom_model_name

                    lora_save_file = os.path.join(loras_dir, f"{lora_model_name}_{args.revision}.safetensors")

                    with accelerator.autocast(), torch.inference_mode():

                        def lora_save_function(weights, filename):
                            metadata = args.export_ss_metadata()
                            logger.debug(f"Saving lora to {filename}")
                            safetensors.torch.save_file(weights, filename, metadata=metadata)

                        if save_lora:
                            # TODO: Add a version for the lora model?
                            pbar2.reset(1)
                            pbar2.set_description("Saving Lora Weights...")
                            # setup directory
                            logger.debug(f"Saving lora to {lora_save_file}")
                            unet_lora_layers_to_save = unet_lora_state_dict(unet)
                            text_encoder_one_lora_layers_to_save = None
                            text_encoder_two_lora_layers_to_save = None
                            if args.stop_text_encoder != 0:
                                text_encoder_one_lora_layers_to_save = text_encoder_lora_state_dict(text_encoder)
                            if args.model_type == "SDXL":
                                if args.stop_text_encoder != 0:
                                    text_encoder_two_lora_layers_to_save = text_encoder_lora_state_dict(text_encoder_two)
                                StableDiffusionXLPipeline.save_lora_weights(
                                    loras_dir,
                                    unet_lora_layers=unet_lora_layers_to_save,
                                    text_encoder_lora_layers=text_encoder_one_lora_layers_to_save,
                                    text_encoder_2_lora_layers=text_encoder_two_lora_layers_to_save,
                                    weight_name=lora_save_file,
                                    safe_serialization=True,
                                    save_function=lora_save_function
                                )
                                scheduler_args = {}

                                if "variance_type" in s_pipeline.scheduler.config:
                                    variance_type = s_pipeline.scheduler.config.variance_type

                                    if variance_type in ["learned", "learned_range"]:
                                        variance_type = "fixed_small"

                                    scheduler_args["variance_type"] = variance_type

                                s_pipeline.scheduler = UniPCMultistepScheduler.from_config(s_pipeline.scheduler.config, **scheduler_args)
                                save_lora = False
                                save_model = False
                            else:
                                StableDiffusionPipeline.save_lora_weights(
                                    loras_dir,
                                    unet_lora_layers=unet_lora_layers_to_save,
                                    text_encoder_lora_layers=text_encoder_one_lora_layers_to_save,
                                    weight_name=lora_save_file,
                                    safe_serialization=True
                                )
                                s_pipeline.scheduler = get_scheduler_class("UniPCMultistep").from_config(
                                    s_pipeline.scheduler.config)
                            s_pipeline.scheduler.config.solver_type = "bh2"
                            save_lora = False
                            save_model = False

                        elif save_diffusers:
                            # æˆ‘ä»¬æ­£åœ¨èŠ‚çœæƒé‡ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿ä¿å­˜ä¿®è®¢
                            if "_tmp" not in weights_dir:
                                args.save()
                            try:
                                out_file = None
                                status.textinfo = (
                                    f"Saving diffusion model at step {args.revision}..."
                                )
                                update_status({"status": status.textinfo})
                                pbar2.reset(1)

                                pbar2.set_description("Saving diffusion model")
                                s_pipeline.save_pretrained(
                                    weights_dir,
                                    safe_serialization=False,
                                )
                                if ema_model is not None:
                                    ema_model.save_pretrained(
                                        os.path.join(
                                            weights_dir,
                                            "ema_unet",
                                        ),
                                        safe_serialization=False,
                                    )
                                pbar2.update()

                                if save_snapshot:
                                    pbar2.reset(1)
                                    pbar2.set_description("Saving Snapshot")
                                    status.textinfo = (
                                        f"Saving snapshot at step {args.revision}..."
                                    )
                                    update_status({"status": status.textinfo})
                                    accelerator.save_state(
                                        os.path.join(
                                            args.model_dir,
                                            "checkpoints",
                                            f"checkpoint-{args.revision}",
                                        )
                                    )
                                    pbar2.update()

                                # æ— è®ºå¦‚ä½•ï¼Œæˆ‘ä»¬éƒ½åº”è¯¥ä¿å­˜å®ƒï¼Œå› ä¸ºå¦‚æœä¸å­˜åœ¨å¿«ç…§ï¼Œè¿™æ˜¯æˆ‘ä»¬çš„å¤‡ç”¨æ–¹æ¡ˆã€‚

                                # å°†ptæ‰“åŒ…åˆ°æ£€æŸ¥ç‚¹ä¸­
                                if save_checkpoint:
                                    pbar2.reset(1)
                                    pbar2.set_description("Compiling Checkpoint")
                                    snap_rev = str(args.revision) if save_snapshot else ""
                                    if export_diffusers:
                                        copy_diffusion_model(args.model_name, os.path.join(user_model_dir, "diffusers"))
                                    else:
                                        if args.model_type == "SDXL":
                                            compile_checkpoint_xl(args.model_name, reload_models=False,
                                                                  lora_file_name=out_file,
                                                                  log=False, snap_rev=snap_rev, pbar=pbar2)
                                        else:
                                            compile_checkpoint(args.model_name, reload_models=False,
                                                               lora_file_name=out_file,
                                                               log=False, snap_rev=snap_rev, pbar=pbar2)
                                    printm("Restored, moved to acc.device.")
                                    pbar2.update()

                            except Exception as ex:
                                logger.warning(f"Exception saving checkpoint/model: {ex}")
                                traceback.print_exc()
                                pass
                        save_dir = args.model_dir

                    if save_image:
                        logger.debug("Saving images...")
                        # Get the path to a temporary directory
                        del s_pipeline
                        logger.debug(f"Loading image pipeline from {weights_dir}...")
                        if args.model_type == "SDXL":
                            s_pipeline = StableDiffusionXLPipeline.from_pretrained(
                                weights_dir, vae=vae, revision=args.revision,
                                torch_dtype=weight_dtype
                            )
                        else:
                            s_pipeline = StableDiffusionPipeline.from_pretrained(
                                weights_dir, vae=vae, revision=args.revision,
                                torch_dtype=weight_dtype
                            )
                            if args.tomesd:
                                tomesd.apply_patch(s_pipeline, ratio=args.tomesd, use_rand=False)
                        if args.use_lora:
                            s_pipeline.load_lora_weights(lora_save_file)

                        try:
                            s_pipeline.enable_vae_tiling()
                            s_pipeline.enable_vae_slicing()
                            s_pipeline.enable_sequential_cpu_offload()
                            s_pipeline.enable_xformers_memory_efficient_attention()
                        except:
                            pass

                        samples = []
                        sample_prompts = []
                        last_samples = []
                        last_prompts = []
                        status.textinfo = (
                            f"Saving preview image(s) at step {args.revision}..."
                        )
                        update_status({"status": status.textinfo})
                        try:
                            s_pipeline.set_progress_bar_config(disable=True)
                            sample_dir = os.path.join(save_dir, "samples")
                            os.makedirs(sample_dir, exist_ok=True)

                            sd = SampleDataset(args)
                            prompts = sd.prompts
                            logger.debug(f"Generating {len(prompts)} samples...")

                            concepts = args.concepts()
                            if args.sanity_prompt:
                                epd = PromptData(
                                    prompt=args.sanity_prompt,
                                    seed=args.sanity_seed,
                                    negative_prompt=concepts[
                                        0
                                    ].save_sample_negative_prompt,
                                    resolution=(args.resolution, args.resolution),
                                )
                                prompts.append(epd)

                            prompt_lengths = len(prompts)
                            if args.disable_logging:
                                pbar2.reset(prompt_lengths)
                            else:
                                pbar2.reset(prompt_lengths + 2)
                            pbar2.set_description("Generating Samples")
                            ci = 0
                            for c in prompts:
                                c.out_dir = os.path.join(args.model_dir, "samples")
                                generator = torch.manual_seed(int(c.seed))
                                s_image = s_pipeline(
                                    c.prompt,
                                    num_inference_steps=c.steps,
                                    guidance_scale=c.scale,
                                    negative_prompt=c.negative_prompt,
                                    height=c.resolution[1],
                                    width=c.resolution[0],
                                    generator=generator,
                                ).images[0]
                                sample_prompts.append(c.prompt)
                                image_name = db_save_image(
                                    s_image,
                                    c,
                                    custom_name=f"sample_{args.revision}-{ci}",
                                )
                                shared.status.current_image = image_name
                                shared.status.sample_prompts = [c.prompt]
                                update_status({"images": [image_name], "prompts": [c.prompt]})
                                samples.append(image_name)
                                pbar2.update()
                                ci += 1
                            for sample in samples:
                                last_samples.append(sample)
                            for prompt in sample_prompts:
                                last_prompts.append(prompt)
                            del samples
                            del prompts
                        except:
                            logger.warning(f"Exception saving sample.")
                            traceback.print_exc()
                            pass

                        del s_pipeline
                        printm("Starting cleanup.")

                        if os.path.isdir(loras_dir) and "_tmp" in loras_dir:
                            shutil.rmtree(loras_dir)

                        if os.path.isdir(weights_dir) and "_tmp" in weights_dir:
                            shutil.rmtree(weights_dir)

                        if "generator" in locals():
                            del generator

                        if not args.disable_logging:
                            try:
                                printm("Parse logs.")
                                log_images, log_names = log_parser.parse_logs(model_name=args.model_name)
                                pbar2.update()
                                for log_image in log_images:
                                    last_samples.append(log_image)
                                for log_name in log_names:
                                    last_prompts.append(log_name)

                                del log_images
                                del log_names
                            except Exception as l:
                                traceback.print_exc()
                                logger.warning(f"Exception parsing logz: {l}")
                                pass

                        send_training_update(
                            last_samples,
                            args.model_name,
                            last_prompts,
                            global_step,
                            args.revision
                        )

                        status.sample_prompts = last_prompts
                        status.current_image = last_samples
                        update_status({"images": last_samples, "prompts": last_prompts})
                        pbar2.update()


                    if args.cache_latents:
                        printm("Unloading vae.")
                        del vae
                        # å†æ¬¡ä¿ç•™å¼•ç”¨
                        vae = None

                    status.current_image = last_samples
                    update_status({"images": last_samples})
                    cleanup()
                    printm("Cleanup.")

                    optim_to(profiler, optimizer, accelerator.device)

                    # æ¢å¤æ‰€æœ‰éšæœºçŠ¶æ€ï¼Œä»¥é¿å…è¿›è¡Œé‡‡æ ·å½±å“è®­ç»ƒã€‚
                    if shared.device.type == 'cuda':
                        torch.set_rng_state(torch_rng_state)
                        torch.cuda.set_rng_state(cuda_cpu_rng_state, device="cpu")
                        torch.cuda.set_rng_state(cuda_gpu_rng_state, device="cuda")

                    cleanup()

                    # å¦‚æœæˆ‘ä»¬è¦ä¿å­˜æ¨¡å‹ï¼Œåˆ™ä¿å­˜loraæƒé‡
                    if os.path.isfile(lora_save_file) and not delete_tmp_lora:
                        meta = args.export_ss_metadata()
                        convert_diffusers_to_kohya_lora(lora_save_file, meta, args.lora_weight)
                    else:
                        if os.path.isfile(lora_save_file):
                            os.remove(lora_save_file)

                    printm("Completed saving weights.")
                    pbar2.reset()

            # åœ¨æ¯å°æœºå™¨ä¸Šåªæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦æ¡ï¼Œå¹¶ä¸”ä¸å°†çŠ¶æ€å‘é€åˆ°æ–°çš„UIã€‚
            progress_bar = mytqdm(
                # å±•ç¤ºè¿­ä»£èŒƒå›´ global_step åˆ° max_train_steps 
                range(global_step, max_train_steps),
                # éæœ¬åœ°è¿è¡Œä¸æ˜¾ç¤ºè¿›åº¦æ¡
                disable=not accelerator.is_local_main_process,
                # æ°¸è¿œæ˜¾ç¤ºåœ¨å‘½ä»¤è¡Œç¬¬ä¸€è¡Œ
                position=0
            )
            # è®¾ç½®è¿›åº¦æ¡æ ‡ç­¾
            progress_bar.set_description("Steps")
            # è¡¨ç¤ºæ›´æ–°ååˆ·æ–°æ˜¾ç¤ºåç¼€
            progress_bar.set_postfix(refresh=True)
            # å°† args.revision æ”¹ä¸º int ç±»å‹
            args.revision = (
                args.revision if isinstance(args.revision, int) else
                int(args.revision) if str(args.revision).strip() else
                0
            )
            lifetime_step = args.revision       # å®šä¹‰å½“å‰ç”Ÿå‘½å‘¨æœŸçš„æ­¥æ•°
            lifetime_epoch = args.epoch         # å®šä¹‰å½“å‰ç”Ÿå‘½å‘¨æœŸçš„è½®æ•°
            status.job_count = max_train_steps  # 
            status.job_no = global_step
            update_status({"progress_1_total": max_train_steps, "progress_1_job_current": global_step})
            training_complete = False
            msg = ""

            last_tenc = 0 < text_encoder_epochs
            if stop_text_percentage == 0:
                last_tenc = False

            cleanup()
            stats = {
                "loss": 0.0,
                "prior_loss": 0.0,
                "instance_loss": 0.0,
                "unet_lr": learning_rate,
                "tenc_lr": txt_learning_rate,
                "session_epoch": 0,                                         # åˆå§‹åŒ–è½®æ•°
                "lifetime_epoch": args.epoch,                               # åˆå§‹åŒ–å½“å‰ç”Ÿå‘½å‘¨æœŸçš„è½®æ•°
                "total_session_epoch": args.num_train_epochs,               # 
                "total_lifetime_epoch": args.epoch + args.num_train_epochs, # è®¡ç®—æ•´ä¸ªè®­ç»ƒç”Ÿå‘½å‘¨æœŸçš„è½®æ•°
                "lifetime_step": args.revision,
                "session_step": 0,
                "total_session_step": max_train_steps,
                "total_lifetime_step": args.revision + max_train_steps,
                "steps_per_epoch": len(train_dataset),
                "iterations_per_second": 0.0,
                "vram": round(torch.cuda.memory_reserved(0) / 1024 ** 3, 1)
            }
            for epoch in range(first_epoch, max_train_epochs):
                if training_complete:
                    logger.debug("Training complete, breaking epoch.")
                    break
                
                # è®­ç»ƒUnet
                if args.train_unet:
                    unet.train()
                elif args.use_lora and not args.lora_use_buggy_requires_grad:
                    set_lora_requires_grad(unet, False)
                    
                # åˆ¤æ–­æ˜¯å¦ç»§ç»­è¿›è¡ŒCLIPè®­ç»ƒ
                train_tenc = epoch < text_encoder_epochs
                if stop_text_percentage == 0:
                    train_tenc = False
                    
                # é€‰æ‹©æ˜¯å¦å†»ç»“clipçš„å½’ä¸€å±‚
                if args.freeze_clip_normalization:
                    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œä¸è®­ç»ƒ
                    text_encoder.eval()
                    if args.model_type == "SDXL":
                        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œä¸è®­ç»ƒ
                        text_encoder_two.eval()
                else:
                    # æ ¹æ® train_tenc åˆ¤æ–­æ˜¯å¦è¿›è¡Œè®­ç»ƒæ¨¡å¼
                    text_encoder.train(train_tenc)
                    if args.model_type == "SDXL":
                        text_encoder_two.train(train_tenc)

                if args.use_lora:
                    if not args.lora_use_buggy_requires_grad:
                        set_lora_requires_grad(text_encoder, train_tenc)
                        # ä¸ºäº†è®©æ¸å˜æ£€æŸ¥ç‚¹å·¥ä½œï¼Œæˆ‘ä»¬éœ€è¦åœ¨è¾“å…¥ä¸Šå¯ç”¨æ¸å˜
                        # è¿™ä¸ä¼šè¢«ä¼˜åŒ–ï¼Œå› ä¸ºå®ƒä¸æ˜¯ä¼˜åŒ–å™¨çš„å‚æ•°
                        text_encoder.text_model.embeddings.position_embedding.requires_grad_(train_tenc)
                        if args.model_type == "SDXL":
                            set_lora_requires_grad(text_encoder_two, train_tenc)
                            text_encoder_two.text_model.embeddings.position_embedding.requires_grad_(train_tenc)
                else:
                    # æ ¹æ® text_encoder è®¾ç½®æ˜¯å¦è¿›è¡Œæ¢¯åº¦è®¡ç®—
                    text_encoder.requires_grad_(train_tenc)
                    if args.model_type == "SDXL":
                        # æ ¹æ® text_encoder è®¾ç½®æ˜¯å¦è¿›è¡Œæ¢¯åº¦è®¡ç®—
                        text_encoder_two.requires_grad_(train_tenc)
                        
                # æ ¹æ®å½“å‰è½®æ•°åˆ¤æ–­æ˜¯å¦ç»§ç»­è®­ç»ƒ
                if last_tenc != train_tenc:
                    last_tenc = train_tenc
                    cleanup()
                    
                # å®šä¹‰æŸå¤±å€¼
                loss_total = 0
                
                # è®¡ç®—å…ˆéªŒæŸå¤±æƒé‡
                current_prior_loss_weight = current_prior_loss(
                    args, current_epoch=global_epoch
                )

                instance_loss = None    # å®ä¾‹æŸå¤±
                prior_loss = None       # å…ˆéªŒæŸå¤±
                
                
                """
                #######################################################################################################
                @@@                                                                                                 @@@
                @@@                                           çœŸæ­£çš„è®­ç»ƒéƒ¨åˆ†                                          @@@
                @@@                                                                                                 @@@
                #######################################################################################################
                """
                
                
                for step, batch in enumerate(train_dataloader):
                    # åˆ¤æ–­æ˜¯å¦è¦ä»æ£€æŸ¥ç‚¹å¼€å§‹è®­ç»ƒ
                    if (
                            resume_from_checkpoint
                            and epoch == first_epoch
                            and step < resume_step
                    ):
                        progress_bar.update(train_batch_size)
                        progress_bar.reset()
                        status.job_count = max_train_steps
                        status.job_no += train_batch_size
                        stats["session_step"] += train_batch_size
                        stats["lifetime_step"] += train_batch_size
                        update_status(stats)
                        continue

                    with ConditionalAccumulator(accelerator, unet, text_encoder, text_encoder_two):
                        # å°†å›¾åƒè½¬æ¢ä¸ºæ½œç©ºé—´
                        with torch.no_grad():       # åœ¨ä¸Šä¸‹æ–‡ä¸è¿›è¡Œæ¢¯åº¦è®¡ç®— 
                            # å¦‚æœå¼€å¯äº†ç¼“å­˜æ½œåœ¨å˜é‡
                            if args.cache_latents:
                                # ç›´æ¥å°†å›¾ç‰‡æ‰”è¿›æ˜¾å¡
                                latents = batch["images"].to(accelerator.device)
                            # æ²¡å¼€å°±ç”¨vaeè¿›è¡Œç¼–ç åå†æ‰”è¿›å»
                            else:
                                latents = vae.encode(
                                    batch["images"].to(dtype=weight_dtype)
                                ).latent_dist.sample()  # ä»æ½œåœ¨åˆ†å¸ƒä¸­éšæœºé‡‡æ ·ä¸€äº›å€¼
                            latents = latents * 0.18215 # å¯¹æ½œåœ¨ç©ºé—´è¿›è¡Œç¼©æ”¾

                        # æˆ‘ä»¬å°†æ·»åŠ åˆ°æ¨¡å‹è¾“å…¥çš„å™ªå£°æ ·æœ¬
                        noise = torch.randn_like(latents, device=latents.device)    # åŠ å™ª
                        # å™ªå£°åç§»å¦‚æœä¸º0(ç®€å•æ¥è¯´æ˜¯æ§åˆ¶ç”»é¢äº®åº¦çš„)
                        if args.offset_noise != 0:
                            # https://www.crosslabs.org//blog/diffusion-with-offset-noise
                            noise += args.offset_noise * torch.randn(
                                (latents.shape[0],
                                 latents.shape[1],
                                 1,
                                 1),
                                device=latents.device
                            )
                        # ä¾æ¬¡ä¼ é€’å‚æ•°
                        b_size, channels, height, width = latents.shape

                        # å¯¹æ¯ä¸ªå›¾åƒéšæœºé‡‡æ ·ä¸€ä¸ªæ—¶é—´æ­¥é•¿
                        timesteps = torch.randint(
                            0,
                            noise_scheduler.config.num_train_timesteps,
                            (b_size,),
                            device=latents.device
                        )
                        timesteps = timesteps.long()

                        # æ ¹æ®æ¯ä¸ªæ—¶é—´æ­¥é•¿çš„å™ªå£°å¤§å°å‘æ½œå‡½æ•°æ·»åŠ å™ªå£°
                        # (è¿™æ˜¯æ­£å‘æ‰©æ•£è¿‡ç¨‹)
                        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)    # è¿›è¡Œä¸€æ¬¡æ€§åŠ å™ªï¼Œè·å¾— noisy_latents æœ€ç»ˆåŠ å™ªç»“æœ
                        # å¦‚æœè®­ç»ƒ CLIP æŸ¥çœ‹æ˜¯å¦è¿›è¡Œæ–‡æœ¬å¡«å……
                        pad_tokens = args.pad_tokens if train_tenc else False
                        input_ids = batch["input_ids"]
                        encoder_hidden_states = None
                        if args.model_type != "SDXL" and text_encoder is not None:
                            # è·å–éšè—çŠ¶æ€
                            encoder_hidden_states = encode_hidden_state(
                                text_encoder,
                                batch["input_ids"],
                                pad_tokens,
                                b_size,
                                args.max_token_length,
                                tokenizer_max_length,
                                args.clip_skip,
                            )
                            
                            
                        # å¦‚æœUnetè¦çš„è¾“å…¥é€šé“å¤§äºVAEçš„æ½œåœ¨ç©ºé—´é€šé“
                        if unet.config.in_channels > channels:
                            # è®¡ç®—å·®å¤šå°‘
                            needed_additional_channels = unet.config.in_channels - channels
                            # éšæœºä¸€ä¸ªå¼ é‡
                            additional_latents = randn_tensor(
                                (b_size, needed_additional_channels, height, width),
                                device=noisy_latents.device,
                                dtype=noisy_latents.dtype,
                            )
                            # å°†éšæœºå‡ºæ¥çš„å¼ é‡å’ŒåŸæ¥çš„å™ªå£°ç›¸åŠ ï¼Œå¾—åˆ°ä¸€ä¸ªæ–°å™ªå£°
                            noisy_latents = torch.cat([additional_latents, noisy_latents], dim=1)
                        # æ ¹æ®é¢„æµ‹ç±»å‹è·å¾—æŸå¤±çš„ç›®æ ‡
                        # epsilon ç›®æ ‡å‡½æ•°ç›´æ¥å’Œå™ªå£°æ‹Ÿåˆ
                        if noise_scheduler.config.prediction_type == "epsilon":
                            target = noise
                        # v_prediction ç›®æ ‡å‡½æ•°æ ¹æ®æ½œåœ¨å˜é‡ã€å™ªå£°å’Œæ—¶é—´æ­¥ï¼Œæ‹Ÿåˆæ½œåœ¨å˜é‡åœ¨æ—¶é—´ä¸Šçš„å˜åŒ–
                        elif noise_scheduler.config.prediction_type == "v_prediction":
                            target = noise_scheduler.get_velocity(latents, noise, timesteps)
                        else:
                            raise ValueError(f"Unknown prediction type {noise_scheduler.config.prediction_type}")

                        if args.model_type == "SDXL":
                            # åœ¨ä¸Šä¸‹æ–‡é‡Œä½¿ç”¨æ··åˆç²¾åº¦è¿›è¡Œè®­ç»ƒ
                            with accelerator.autocast():
                                model_pred = unet(
                                    noisy_latents, timesteps, batch["input_ids"],
                                    added_cond_kwargs=batch["unet_added_conditions"]
                                ).sample
                        else:
                            # é¢„æµ‹å™ªå£°æ®‹å·®å¹¶è®¡ç®—æŸå¤±
                            model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample

                        if args.model_type != "SDXL":
                            # å¾…åŠäº‹é¡¹:è®¾ç½®ä¸€ä¸ªä¼˜å…ˆä¿å­˜æ ‡å¿—ï¼Œå¹¶ä½¿ç”¨å®ƒæ¥ç¡®ä¿è¿™åªå‘ç”Ÿåœ¨dreamboothä¸­
                            if not args.split_loss and not with_prior_preservation:
                                # ä½¿ç”¨å‡æ–¹è¯¯å·®è®¡ç®—æŸå¤±
                                loss = instance_loss = torch.nn.functional.mse_loss(model_pred.float(), target.float(), reduction="mean")
                                # 
                                loss *= batch["loss_avg"]
                            else:
                                # é¢„æµ‹å™ªå£°æ®‹å·®
                                if model_pred.shape[1] == 6:
                                    model_pred, _ = torch.chunk(model_pred, 2, dim=1)

                                if model_pred.shape[0] > 1 and with_prior_preservation:
                                        # å°†å™ªå£°å’Œmodel_predåˆ†æˆä¸¤éƒ¨åˆ†ï¼Œåˆ†åˆ«è®¡ç®—æ¯ä¸ªéƒ¨åˆ†çš„æŸå¤±ã€‚
                                        print("model shape:")
                                        print(model_pred.shape)
                                        model_pred, model_pred_prior = torch.chunk(model_pred, 2, dim=0)
                                        target, target_prior = torch.chunk(target, 2, dim=0)

                                        # è®¡ç®—å®ä¾‹æŸå¤±
                                        loss = instance_loss = F.mse_loss(model_pred.float(), target.float(), reduction="mean")

                                        # è®¡ç®—å…ˆéªŒæŸå¤±
                                        prior_loss = F.mse_loss(model_pred_prior.float(), target_prior.float(),
                                                                reduction="mean")
                                else:
                                    # è®¡ç®—æŸå¤±
                                    loss = instance_loss = F.mse_loss(model_pred.float(), target.float(), reduction="mean")
                        else:
                            if with_prior_preservation:
                                # å°†å™ªå£°å’Œmodel_predåˆ†æˆä¸¤éƒ¨åˆ†ï¼Œåˆ†åˆ«è®¡ç®—æ¯ä¸ªéƒ¨åˆ†çš„æŸå¤±ã€‚
                                model_pred, model_pred_prior = torch.chunk(model_pred, 2, dim=0)
                                target, target_prior = torch.chunk(target, 2, dim=0)

                                # è®¡ç®—å®ä¾‹ä¸¢å¤±
                                loss = F.mse_loss(model_pred.float(), target.float(), reduction="mean")

                                # è®¡ç®—å…ˆéªŒæŸå¤±
                                prior_loss = F.mse_loss(model_pred_prior.float(), target_prior.float(), reduction="mean")

                                # å°†ä¹‹å‰çš„æŸå¤±æ·»åŠ åˆ°å®ä¾‹æŸå¤±ä¸­ã€‚
                                loss = loss + args.prior_loss_weight * prior_loss   # æŸå¤± = å®ä¾‹æŸå¤± + æŸå¤±æƒé‡ * å…ˆéªŒæŸå¤±
                            else:
                                loss = F.mse_loss(model_pred.float(), target.float(), reduction="mean")
                                
                        # åå‘ä¼ æ’­
                        accelerator.backward(loss)
                        
                        # å¦‚æœå¯ç”¨äº†æ¢¯åº¦åŒæ­¥å¹¶ä¸”æ²¡æœ‰ä½¿ç”¨ Lora åˆ™è¿›è¡Œæ¢¯åº¦è£å‰ª
                        if accelerator.sync_gradients and not args.use_lora:
                            if train_tenc:
                                if args.model_type == "SDXL":
                                    params_to_clip = itertools.chain(unet.parameters(), text_encoder.parameters(),
                                                                     text_encoder_two.parameters())
                                else:
                                    params_to_clip = itertools.chain(unet.parameters(), text_encoder.parameters())
                            else:
                                params_to_clip = unet.parameters()
                            accelerator.clip_grad_norm_(params_to_clip, 1)
                            
                        # æ›´æ–°å‚æ•°
                        optimizer.step()
                        # æ›´æ–°å­¦ä¹ ç‡
                        lr_scheduler.step(train_batch_size)
                        # æ›´æ–° emaæ¨¡å‹ å‚æ•°
                        if args.use_ema and ema_model is not None:
                            ema_model.step(unet)
                        # è¿›è¡Œæ€§èƒ½åˆ†æ
                        if profiler is not None:
                            profiler.step()
                            
                        # åˆ¤æ–­æ˜¯å°†æ¢¯åº¦æ¸…é›¶è¿˜æ˜¯ä¸ºç©º
                        optimizer.zero_grad(set_to_none=args.gradient_set_to_none)
                        
                    # è¿”å›ä½¿ç”¨çš„æ˜¾å­˜
                    allocated = round(torch.cuda.memory_allocated(0) / 1024 ** 3, 1)
                    # è¿”å›å½“å‰æ‰€æœ‰çš„æ˜¾å­˜
                    cached = round(torch.cuda.memory_reserved(0) / 1024 ** 3, 1)
                    # è¿”å›æœ€åä¸€ä¸ªå­¦ä¹ ç‡
                    lr_data = lr_scheduler.get_last_lr()
                    # å°†æœ€åä¸€ä¸ªå­¦ä¹ ç‡èµ‹å€¼ç»™ last_lr 
                    last_lr = lr_data[0]
                    # åˆå§‹åŒ–æœ€åä¸€ä¸ªæ–‡æœ¬å­¦ä¹ ç‡
                    last_tenc_lr = 0
                    # ä¸º stats æ·»åŠ ä¸€ä¸ªé”®å€¼å¯¹
                    stats["lr_data"] = lr_data
                    # è·å–æ–‡æœ¬ç¼–ç å™¨çš„å­¦ä¹ ç‡
                    try:
                        if len(optimizer.param_groups) > 1:
                            last_tenc_lr = optimizer.param_groups[1]["lr"] if train_tenc else 0
                    except:
                        logger.debug("Exception getting tenc lr")
                        pass
                    
                    
                    if 'adapt' in args.optimizer:
                        last_lr = optimizer.param_groups[0]["d"] * optimizer.param_groups[0]["lr"]
                        if len(optimizer.param_groups) > 1:
                            try:
                                # è®¡ç®—è¡°å‡åçš„æ–‡æœ¬ç¼–ç å™¨å­¦ä¹ ç‡
                                last_tenc_lr = optimizer.param_groups[1]["d"] * optimizer.param_groups[1]["lr"]
                            except:
                                logger.warning("Exception setting tenc weight decay")
                                traceback.print_exc()

                    update_status(stats)
                    del latents
                    del encoder_hidden_states
                    del noise
                    del timesteps
                    del noisy_latents
                    del target
                    
                    # æ›´æ–°å…¨å±€è¿›åº¦å‚æ•°
                    global_step += train_batch_size
                    args.revision += train_batch_size
                    status.job_no += train_batch_size
                    # å°†æŸå¤±ä»è®¡ç®—å›¾ä¸­åˆ†ç¦»å‡ºæ¥ï¼Œå¹¶è½¬æ¢ä¸ºæ ‡é‡
                    loss_step = loss.detach().item()
                    # è·Ÿè¸ªè®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å€¼
                    loss_total += loss_step
                    
                    # æ›´æ–°çŠ¶æ€æ å­—æ®µ
                    stats["session_step"] += train_batch_size
                    stats["lifetime_step"] += train_batch_size
                    stats["loss"] = loss_step

                    logs = {
                        "lr": float(last_lr),
                        "loss": float(loss_step),
                        "vram": float(cached),
                    }
                    
                    # ä¸ºçŠ¶æ€æ æ·»åŠ æ˜¾ç¤ºå­—æ®µ
                    stats["vram"] = logs["vram"]
                    stats["unet_lr"] = '{:.2E}'.format(Decimal(last_lr))
                    stats["tenc_lr"] = '{:.2E}'.format(Decimal(last_tenc_lr))

                    if args.split_loss and with_prior_preservation and args.model_type != "SDXL":
                        # ä¸º logs æ–°æ·»å®ä¾‹æŸå¤±å­—æ®µ
                        logs["inst_loss"] = float(instance_loss.detach().item())
                        
                        # ä¸º logs æ–°æ·»å…ˆéªŒæŸå¤±å­—æ®µ
                        if prior_loss is not None:
                            logs["prior_loss"] = float(prior_loss.detach().item())
                        else:
                            logs["prior_loss"] = None  # æˆ–è€…å…¶ä»–é»˜è®¤å€¼
                            
                        # ä¸ºçŠ¶æ€æ æ·»åŠ æ˜¾ç¤ºå­—æ®µ
                        stats["instance_loss"] = logs["inst_loss"]
                        stats["prior_loss"] = logs["prior_loss"]

                    if 'adapt' in args.optimizer:
                        status.textinfo2 = (
                            f"Loss: {'%.2f' % loss_step}, UNET DLR: {'{:.2E}'.format(Decimal(last_lr))}, TENC DLR: {'{:.2E}'.format(Decimal(last_tenc_lr))}, "
                            f"VRAM: {allocated}/{cached} GB"
                        )
                    else:
                        status.textinfo2 = (
                            f"Loss: {'%.2f' % loss_step}, LR: {'{:.2E}'.format(Decimal(last_lr))}, "
                            f"VRAM: {allocated}/{cached} GB"
                        )

                    progress_bar.update(train_batch_size)
                    
                    # å¦‚æœ mytqdm é‡Œæœ‰ rate å°±è¿›è¡Œèµ‹å€¼ï¼Œæ²¡æœ‰å°±è®¾ç½®ä¸ºNone
                    rate = progress_bar.format_dict["rate"] if "rate" in progress_bar.format_dict else None
                    if rate is None:
                        rate_string = ""
                    else:
                        # é€‰æ‹©æ˜¾ç¤ºé€Ÿç‡çš„æ–¹å¼
                        if rate > 1:
                            rate_string = f"{rate:.2f} it/s"
                        else:
                            rate_string = f"{1 / rate:.2f} s/it" if rate != 0 else "N/A"
                    # ä¸ºçŠ¶æ€æ æ·»åŠ é€Ÿç‡å­—æ®µ
                    stats["iterations_per_second"] = rate_string
                    progress_bar.set_postfix(**logs)
                    accelerator.log(logs, step=args.revision)

                    logs = {"epoch_loss": loss_total / len(train_dataloader)}
                    accelerator.log(logs, step=global_step)
                    stats["epoch_loss"] = '%.2f' % (loss_total / len(train_dataloader))

                    status.job_count = max_train_steps
                    status.job_no = global_step
                    stats["lifetime_step"] = args.revision
                    stats["session_step"] = global_step
                    # status0 = f"Steps: {global_step}/{max_train_steps} (Current), {rate_string}"
                    # status1 = f"{args.revision}/{lifetime_step + max_train_steps} (Lifetime), Epoch: {global_epoch}"
                    status.textinfo = (
                        f"Steps: {global_step}/{max_train_steps} (Current), {rate_string}"
                        f" {args.revision}/{lifetime_step + max_train_steps} (Lifetime), Epoch: {global_epoch}"
                    )
                    update_status(stats)
                    
                    # æ£€æµ‹ loss_step æ˜¯å¦ä¸º NaNï¼ˆNot a Numberï¼‰ï¼Œå¦‚æœæ˜¯å°±ä¸­æ­¢è®­ç»ƒ
                    if math.isnan(loss_step):
                        logger.warning("Loss is NaN, your model is dead. Cancelling training.")
                        status.interrupted = True
                        if status_handler:
                            status_handler.end("Training interrrupted due to NaN loss.")

                    # æ—¥å¿—å®Œæˆæ¶ˆæ¯
                    if training_complete or status.interrupted:
                        # ä¸è¿›è¡Œä¸­æ­¢
                        shared.in_progress = False
                        # å°†è½®æ•°å’Œæ­¥æ•°æ¸…é›¶
                        shared.in_progress_step = 0
                        shared.in_progress_epoch = 0
                        logger.debug("  Training complete (step check).")
                        # æ ¹æ®æ˜¯å¦ä¸­æ­¢ç¨‹åºåˆ¤æ–­å½“å‰çŠ¶æ€
                        if status.interrupted:
                            state = "canceled"
                        else:
                            state = "complete"
                            
                        # è®­ç»ƒçŠ¶æ€
                        status.textinfo = (
                            f"Training {state} {global_step}/{max_train_steps}, {args.revision}"
                            f" total."
                        )
                        if status_handler:
                            status_handler.end(status.textinfo)
                        break
                    
                """
                #######################################################################################################
                @@@                                                                                                 @@@
                @@@                                     ä¸€æ‰¹æ¬¡è®­ç»ƒéƒ¨åˆ†ç»“æŸ                                            @@@
                @@@                                                                                                 @@@
                #######################################################################################################
                """
                
                
                
                # ç­‰å¾…æ¯ä¸ªè¿›ç¨‹ç»“æŸ
                accelerator.wait_for_everyone()
                
                # è¿›é¡¹å‚æ•°æ›´æ–°
                args.epoch += 1
                global_epoch += 1
                lifetime_epoch += 1
                session_epoch += 1
                stats["session_epoch"] += 1
                stats["lifetime_epoch"] += 1
                lr_scheduler.step(is_epoch=True)
                status.job_count = max_train_steps
                status.job_no = global_step
                update_status(stats)
                # è¿è¡Œä¿å­˜å‡½æ•°
                check_save(True)

                if args.num_train_epochs > 1:
                    training_complete = session_epoch >= max_train_epochs

                if training_complete or status.interrupted:
                    logger.debug("  Training complete (step check).")
                    if status.interrupted:
                        state = "canceled"
                    else:
                        state = "complete"

                    status.textinfo = (
                        f"Training {state} {global_step}/{max_train_steps}, {args.revision}"
                        f" total."
                    )
                    if status_handler:
                        status_handler.end(status.textinfo)
                    break

                # åœ¨æ—¶ä»£çš„æœ€ååšè¿™ä»¶äº‹ï¼Œåœ¨æˆ‘ä»¬ç¡®å®šè¿˜æ²¡æœ‰å®Œæˆä¹‹å
                if args.epoch_pause_frequency > 0 and args.epoch_pause_time > 0:
                    if not session_epoch % args.epoch_pause_frequency:
                        logger.debug(
                            f"Giving the GPU a break for {args.epoch_pause_time} seconds."
                        )
                        for i in range(args.epoch_pause_time):
                            if status.interrupted:
                                training_complete = True
                                logger.debug("Training complete, interrupted.")
                                if status_handler:
                                    status_handler.end("Training interrrupted.")
                                break
                            time.sleep(1)
            """
                #######################################################################################################
                @@@                                                                                                 @@@
                @@@                                     å…¨éƒ¨è®­ç»ƒéƒ¨åˆ†ç»“æŸ                                              @@@
                @@@                                                                                                 @@@
                #######################################################################################################
            """

            cleanup_memory()
            accelerator.end_training()
            result.msg = msg
            result.config = args
            result.samples = last_samples
            stop_profiler(profiler)
            return result

    return inner_loop()
